from IPython.display import update_display
update_display(None, display_id='update_display')
update_display({'application/vnd.vegalite.v4+json': 104857600000}, display_id='update_display')
#increasing the input size for visualization of jason file
from google.colab import drive
drive.mount('/content/drive')
#training datase
import random
import json
import pickle
import numpy as np
import pandas as pd
!pip install keras==2.11.0
!pip install tensorflow==2.11.*
import nltk
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Activation,Dropout
from tensorflow.keras.optimizers.legacy import SGD
from tensorflow.keras.models import load_model #testing purpose alone
lemmatizer=WordNetLemmatizer()
from google.colab import drive
drive.mount('/content/gdrive')
import json
with open('/content/gdrive/MyDrive/jokes.JSON') as f:
    intents = json.load(f)
#preprocessing dataset using nltk
words=[]
classes=[]
documents=[]
ignore_letters=['?','!','.',',']#punctuations are ignored
for intent in intents['intents']:
  for pattern in intent['patterns']:
    word_list=nltk.word_tokenize(pattern)
    words.extend(word_list)
    documents.append((word_list,intent['tag']))
    if intent['tag'] not in classes:
      classes.append(intent['tag'])
words =[lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]#converts tto natural form like helping to help
words = sorted(set(words))
classes=sorted(set(classes))
pickle.dump(words,open('/content/gdrive/MyDrive/words.pkl','wb'))#converted tto byte stream.
pickle.dump(classes,open('/content/gdrive/MyDrive/classes.pkl','wb'))

training=[]
output_empty=[0]*len(classes)#create a list of 0,s length=number classes in dataset.

for document in documents:
  bag=[]
  word_patterns=document[0]
  words = [lemmatizer.lemmatize(word) for word in words if word and word not in ignore_letters]
  for word in words:
    bag.append(1) if word in word_patterns else bag.append(0)

  output_row=list(output_empty)#copy of output_empty
  output_row[classes.index(document[1])]=1
  training.append([bag,output_row])

random.shuffle(training)
training=np.array(training)

train_x=list(training[:,0])
train_y=list(training[:,1])
model=Sequential()
model.add(Dense(128,input_shape=(len(train_x[0]),),activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]),activation='softmax'))


sgd=SGD(learning_rate=0.01,decay=1e-6,momentum=0.9,nesterov=True)#stochiastic graient descen- to control loss function uring training
model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])
hist = model.fit(np.array(train_x),np.array(train_y),epochs=200,batch_size=5,verbose=1)#epoch -number of times training iterates over data seta in batches
model.save('/content/gdrive/MyDrive/chatbotmodel.h5', hist)
print('TrainingÂ Done')
# testing
with open('/content/gdrive/MyDrive/jokes.JSON') as json_file:
    intents = json.load(json_file)

words=pickle.load(open('/content/gdrive/MyDrive/words.pkl','rb'))
classes=pickle.load(open('/content/gdrive/MyDrive/classes.pkl','rb'))
model=load_model('/content/gdrive/MyDrive/chatbotmodel.h5')

def clean_up_sentence(sentence):
  sentence_words=nltk.word_tokenize(sentence)
  sentence_words=[lemmatizer.lemmatize(word) for word in sentence_words]
  return sentence_words
def bag_of_words(sentence):
  sentence_words=clean_up_sentence(sentence)
  bag=[0]*len(words)
  for w in sentence_words:
    for i,word in enumerate(words):
      if word == w:
        bag[i]=1
  return np.array(bag)

def predict_class(sentence):
  bow=bag_of_words(sentence)
  res=model.predict(np.array([bow]))[0]
  ERROR_THRESHOLD=0.25
  results=[[i,r] for i,r in enumerate(res) if r> ERROR_THRESHOLD]

  results.sort(key=lambda x:x[1],reverse=True)
  return_list=[]
  for r in results:
    return_list.append({'intent': classes[r[0]],'probability':str(r[1])})
  return return_list
def get_response(intents_list,intents_json):
  tag=intents_list[0]['intent']
  list_of_intents=intents_json['intents']
  for i in list_of_intents:
    if i['tag']==tag:
      result=random.choice(i['responses'])
      break
  return result

print("Hi \nhow,may i help you.")
while True:
  message = input("")
  if message == "bye":
    ints = predict_class(message)
    print(get_response(ints, intents))
    break
  else:
    ints = predict_class(message)
    if len(ints) > 0:
      print(get_response(ints, intents))
    else:
      print("I'm sorry, I don't understand. Can you please rephrase your question?")
    